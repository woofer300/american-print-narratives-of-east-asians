{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "Configure variables",
   "id": "835408f4a0fc8682",
   "outputs": null,
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T20:41:15.383460Z",
     "start_time": "2024-07-15T20:41:13.686948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import spacy\n",
    "import math\n",
    "from datetime import date\n",
    "import webbrowser\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from pytesseract import pytesseract\n",
    "import json\n",
    "\n",
    "\n",
    "# Configuration variables\n",
    "g_keywords = [\"chinese\", \"chinaman\", \"chink\"]\n",
    "g_search_date_range_start = date(1850, 1, 1)\n",
    "g_search_date_range_end = date(2024, 7, 9)\n",
    "g_num_articles_per_time_period = 50\n",
    "g_time_block_range_years = 5\n",
    "\n",
    "g_search_date_range_days = (g_search_date_range_end - g_search_date_range_start).days\n",
    "g_num_time_blocks = math.ceil(g_search_date_range_days / (365 * g_time_block_range_years))\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "\n",
    "def get_time_block_number(year, search_date_range_start, time_block_range_years):\n",
    "    return math.floor((year - search_date_range_start.year) / time_block_range_years)\n",
    "\n",
    "\n",
    "def get_starting_date(time_block_num, search_date_range_start, time_block_range_years):\n",
    "    return date(search_date_range_start.year + time_block_num * time_block_range_years, 1, 1)\n",
    "\n",
    "\n",
    "def get_ending_date(time_block_num, search_date_range_start, search_date_range_end, time_block_range_years):\n",
    "    ending_date = date(search_date_range_start.year + (time_block_num + 1) * time_block_range_years - 1, 12, 31)\n",
    "    if ending_date > search_date_range_end:\n",
    "        return search_date_range_end\n",
    "    else:\n",
    "        return ending_date\n",
    "\n",
    "\n",
    "def get_keywords_with_num_hits(keywords, start_date, end_date):\n",
    "    keywords_with_num_hits = {}\n",
    "    for keyword in keywords:\n",
    "        webbrowser.open(\"https://www.newspapers.com/search/results/?country=us&date-end=\" + end_date.strftime('%Y-%m-%d') + \"&date-start=\" + start_date.strftime('%Y-%m-%d') + \"&entity-types=page&keyword=\" + keyword)\n",
    "        keywords_with_num_hits[keyword] = int(input(\"Number of hits: \").replace(\",\", \"\"))\n",
    "    return keywords_with_num_hits\n",
    "\n",
    "\n",
    "def get_keywords_with_num_articles(keywords_with_num_hits, num_articles_per_time_period):\n",
    "    total_hits = sum(keywords_with_num_hits.values())\n",
    "    article_remainders = []\n",
    "    keywords_with_num_articles = {}\n",
    "    for keyword, hits in keywords_with_num_hits.items():\n",
    "        keywords_with_num_articles[keyword] = int((hits / total_hits) * num_articles_per_time_period)\n",
    "        article_remainders.append((keyword, ((hits / total_hits) * num_articles_per_time_period) % 1))\n",
    "    article_remainders.sort(key=lambda keyword_with_article_remainder: keyword_with_article_remainder[1], reverse=True)\n",
    "    num_searches_short = num_articles_per_time_period - sum(keywords_with_num_articles.values())\n",
    "    for i in range(num_searches_short):\n",
    "        keywords_with_num_articles[article_remainders[i][0]] += 1\n",
    "    return keywords_with_num_articles\n",
    "\n",
    "\n",
    "def get_keywords_with_urls_and_num_articles(keywords_with_num_articles, start_date, end_date):\n",
    "    keywords_with_urls_and_num_articles = []\n",
    "    for keyword, num_articles in keywords_with_num_articles.items():\n",
    "        keywords_with_urls_and_num_articles.append({\n",
    "            \"keyword\": keyword,\n",
    "            \"url\": \"https://www.newspapers.com/search/results/?country=us&date-end=\" + end_date.strftime('%Y-%m-%d') + \"&date-start=\" + start_date.strftime('%Y-%m-%d') + \"&entity-types=page&keyword=\" + keyword,\n",
    "            \"num_articles\": num_articles\n",
    "        })\n",
    "    return keywords_with_urls_and_num_articles\n",
    "\n",
    "\n",
    "def run_article_download_helper(keywords_with_urls_and_num_articles):\n",
    "    for keyword_with_urls_and_num_articles in keywords_with_urls_and_num_articles:\n",
    "        text_input = input(\"Please download \" + str(keyword_with_urls_and_num_articles[\"num_articles\"]) + \" articles with the keyword \\\"\" + keyword_with_urls_and_num_articles[\"keyword\"] + \"\\\". Press return to continue. Type \\\"stop\\\" to stop.\")\n",
    "        if text_input == \"stop\":\n",
    "            break\n",
    "        webbrowser.open(keyword_with_urls_and_num_articles[\"url\"])\n",
    "        text_input = input(\"Press return to continue. Type \\\"stop\\\" to stop.\")\n",
    "        if text_input == \"stop\":\n",
    "            break\n",
    "\n",
    "\n",
    "def get_contexts_for_keyword_from_text(text, keyword):\n",
    "    contexts = []\n",
    "    sentences = list(nlp(text).sents)\n",
    "    for num in range(len(sentences) - 2):\n",
    "        sentence = sentences[num + 1]\n",
    "        if keyword in sentence.text.lower():\n",
    "            contexts.append(sentences[num].text + \" \" + sentence.text + \" \" + sentences[num + 2].text)\n",
    "    return contexts            \n",
    "            \n",
    "      \n",
    "def get_all_contexts_for_keywords(articles_dir_path):\n",
    "    keywords_with_contexts = {}\n",
    "    for articles_by_keyword_dir_path in articles_dir_path.rglob(\"*\"):\n",
    "        if articles_by_keyword_dir_path.is_dir():\n",
    "            keyword = articles_by_keyword_dir_path.name\n",
    "            keywords_with_contexts[keyword] = []\n",
    "            for article_path in articles_by_keyword_dir_path:\n",
    "                text = pytesseract.image_to_string(Image.open(article_path))\n",
    "                date_components = article_path.stem.split(\"-\")\n",
    "                date_of_text = date(int(date_components[0]), int(date_components[1]), int(date_components[2]))\n",
    "                keywords_with_contexts[keyword].append({\n",
    "                    'date': date_of_text,\n",
    "                    'contexts': get_contexts_for_keyword_from_text(text, keyword)\n",
    "                })\n",
    "    return keywords_with_contexts\n",
    "\n",
    "\n",
    "def narrow_contexts(contexts_for_keywords):\n",
    "    narrowed_contexts_per_keyword = {}\n",
    "    for keyword, all_dates_and_contexts in contexts_for_keywords.items():\n",
    "        narrowed_contexts_per_keyword[keyword] = []\n",
    "        for date_and_article_contexts in all_dates_and_contexts:\n",
    "            while True:\n",
    "                if len(date_and_article_contexts) == 0:\n",
    "                    narrowed_contexts_per_keyword[keyword].append({\n",
    "                        'date': date_and_article_contexts[\"date\"],\n",
    "                        'context': \"Placeholder\"})\n",
    "                    break\n",
    "                else:\n",
    "                    random_index = random.randint(0, len(date_and_article_contexts) - 1)\n",
    "                    article_context = date_and_article_contexts['contexts'][random_index]\n",
    "                    print(article_context)\n",
    "                    should_use_context = input(\"Use this context? (y/n or stop)\")\n",
    "                    if should_use_context == \"y\":\n",
    "                        narrowed_contexts_per_keyword[keyword].append({\n",
    "                            'date': date_and_article_contexts[\"date\"],\n",
    "                            'context': article_context})\n",
    "                        return None\n",
    "                    elif should_use_context == \"stop\":\n",
    "                        break\n",
    "                    else:\n",
    "                        date_and_article_contexts['contexts'].pop(random_index)\n",
    "    return narrowed_contexts_per_keyword\n",
    "\n",
    "\n",
    "def write_contexts_to_file(contexts, time_block_number):\n",
    "    if g_narrowed_contexts_per_keyword is not None:\n",
    "        with open('contexts/time_period_' + g_time_block_number + \"_contexts.json\", 'w+') as contexts_file:\n",
    "            json.dump(g_narrowed_contexts_per_keyword, contexts_file)\n",
    "    \n",
    "\n",
    "print(\"Number of time blocks: \" + str(g_num_time_blocks))\n",
    "print(\"Number of articles to download: \" + str(g_num_time_blocks * g_num_articles_per_time_period))\n",
    "\n",
    "g_time_block_number = int(input(\"Pick time block, 0-\" + str(g_num_time_blocks - 1) + \": \"))\n",
    "if g_time_block_number < 0 or g_time_block_number >= g_num_time_blocks:\n",
    "    print(\"Invalid time block!\")\n",
    "    exit()\n",
    "\n",
    "g_start_date = get_starting_date(g_time_block_number, g_search_date_range_start, g_time_block_range_years)\n",
    "g_end_date = get_ending_date(g_time_block_number, g_search_date_range_start, g_search_date_range_end, g_time_block_range_years)\n",
    "\n",
    "print(\"Start date: \" + g_start_date.strftime('%Y-%m-%d'))\n",
    "print(\"End date: \" + g_end_date.strftime('%Y-%m-%d'))\n",
    "print(\"Length of time period in days: \" + str((g_end_date - g_start_date).days))"
   ],
   "id": "bee83284d750dae9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of time blocks: 35\n",
      "Number of articles to download: 1750\n",
      "Start date: 1850-01-01\n",
      "End date: 1854-12-31\n",
      "Length of time period in days: 1825\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Get the number of articles needed for each keyword by pasting in the number of hits for each keyword",
   "id": "43e03d19d1b347bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T20:41:26.957943Z",
     "start_time": "2024-07-15T20:41:18.157321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "g_keywords_with_num_articles = get_keywords_with_num_articles(get_keywords_with_num_hits(g_keywords, g_start_date, g_end_date), g_num_articles_per_time_period)\n",
    "print(g_keywords_with_num_articles)"
   ],
   "id": "a11c28d3daf609a6",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m g_keywords_with_num_articles \u001B[38;5;241m=\u001B[39m get_keywords_with_num_articles(\u001B[43mget_keywords_with_num_hits\u001B[49m\u001B[43m(\u001B[49m\u001B[43mg_keywords\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mg_start_date\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mg_end_date\u001B[49m\u001B[43m)\u001B[49m, g_num_articles_per_time_period)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(g_keywords_with_num_articles)\n",
      "Cell \u001B[0;32mIn[13], line 32\u001B[0m, in \u001B[0;36mget_keywords_with_num_hits\u001B[0;34m(keywords, start_date, end_date)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m keyword \u001B[38;5;129;01min\u001B[39;00m keywords:\n\u001B[1;32m     31\u001B[0m     webbrowser\u001B[38;5;241m.\u001B[39mopen(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://www.newspapers.com/search/results/?country=us&date-end=\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m end_date\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m&date-start=\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m start_date\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m&entity-types=page&keyword=\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m keyword)\n\u001B[0;32m---> 32\u001B[0m     keywords_with_num_hits[keyword] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mNumber of hits: \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplace\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m,\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m keywords_with_num_hits\n",
      "\u001B[0;31mValueError\u001B[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Generate the URLs to grab the articles from",
   "id": "86f2d40b0e7c3624"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T20:40:02.665775Z",
     "start_time": "2024-07-15T20:40:02.662229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "g_keywords_with_urls_and_num_articles = get_keywords_with_urls_and_num_articles(g_keywords_with_num_articles, g_start_date, g_end_date)\n",
    "print(\"Generated \" + str(len(g_keywords_with_urls_and_num_articles)) + \" URLs\")"
   ],
   "id": "99eccf47094edb2c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 URLs\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Utility to guide users in downloading articles. Place articles in a folder with the same name as the keyword, which will be in the \"articles\" folder. Name the file with the date in the format \"YYYY-MM-DD\"",
   "id": "9fde9da22253ab7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T20:40:31.008615Z",
     "start_time": "2024-07-15T20:40:04.953104Z"
    }
   },
   "cell_type": "code",
   "source": "run_article_download_helper(g_keywords_with_urls_and_num_articles)",
   "id": "804d714547350bd3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Get contexts surrounding keywords. Example value for g_contexts_for_keywords:\n",
    "{ 'chinese': [{ 'date': date(2021, 7, 9), 'contexts': [\"Sentence 1\", \"Sentence 2\"]}], 'japanese': [{ 'date': date(2019, 2, 23), 'contexts': [\"Sentence 3\", \"Sentence 4\"]}] "
   ],
   "id": "d5cb5995791464e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "g_articles_dir_path = Path.cwd() / 'articles'\n",
    "g_contexts_for_keywords = get_all_contexts_for_keywords(g_articles_dir_path)"
   ],
   "id": "ee4c4df77f465eab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Narrow down contexts to only on per article. Contexts are presented to the user in a random order, and the first one that the user confirms is relevant is used. If none are confirmed, the context is replaced with a placeholder. Results are stored in a JSON file that should be manually edited to fix placeholders before moving on to analysis.",
   "id": "40fb856214faa28c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "g_narrowed_contexts_per_keyword = narrow_contexts(g_contexts_for_keywords)\n",
    "write_contexts_to_file(g_narrowed_contexts_per_keyword, g_time_block_number)"
   ],
   "id": "d03834a43af9fe08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
